{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import dill\n",
    "from itertools import permutations, combinations\n",
    "import json\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import little_mallet_wrapper as lmw\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018\n",
    "\n",
    "# for each paper -- run once, see which words are not in the dictionary,\n",
    "# add valid missing words to dictionary\n",
    "# run again to confirm caught appropriate missing words\n",
    "# set finish_processing=True to write the cleaned file\n",
    "def clean_paper_2018(papername, paper_path, finish_processing=False):\n",
    "    full_filename = os.path.join(papers_path_root + \"2018\", papername)\n",
    "    # checking if it is a file, cleaning part done in one place\n",
    "    if os.path.isfile(full_filename):\n",
    "        # open file for reading\n",
    "        f = open(full_filename, 'r')\n",
    "        file_contents = f.read()\n",
    "        # for simplicity of running, make sure that after process once,\n",
    "        # can re-process/ is idempotent\n",
    "        \n",
    "        # remove proceedings info from the file; from \"Proceedings\" through \"ABSTRACT\"\n",
    "        cleaned_paper = file_contents\n",
    "        if \"Proceedings\" in cleaned_paper: # TODO make sure case insensitive\n",
    "            res = cleaned_paper.split(\"Proceedings\")\n",
    "            # keep title\n",
    "            title = res[0]\n",
    "            # remove authors, editors, proceedings info\n",
    "            if \"ABSTRACT\" in cleaned_paper: # TODO make sure case insensitive\n",
    "                res = res[1].split(\"ABSTRACT\")\n",
    "                if title:\n",
    "                    cleaned_paper = title + res[1]\n",
    "                else:\n",
    "                    cleaned_paper = res[1]\n",
    "        \n",
    "        # remove acknowledgements\n",
    "        if 'Acknowledgments' in cleaned_paper:       \n",
    "            res = cleaned_paper.split('Acknowledgments')\n",
    "            cleaned_paper = res[0]\n",
    "        elif 'Acknowledgements'in cleaned_paper:\n",
    "            res = cleaned_paper.split('Acknowledgements')\n",
    "            cleaned_paper = res[0]\n",
    "        elif 'ACKNOWLEDGEMENTS' in cleaned_paper:\n",
    "            res = cleaned_paper.split('ACKNOWLEDGMENTS')\n",
    "            cleaned_paper = res[0]\n",
    "        elif 'ACKNOWLEDGEMENTS' in cleaned_paper:\n",
    "            res = cleaned_paper.split('ACKNOWLEDGMENTS')\n",
    "            cleaned_paper = res[0]\n",
    "        \n",
    "        # remove references, if the above didn't get them when removed acknowledgements\n",
    "        if 'References' in cleaned_paper:       \n",
    "            res = cleaned_paper.split('References')\n",
    "            cleaned_paper = res[0]\n",
    "        elif 'REFERENCES'in cleaned_paper:\n",
    "            res = cleaned_paper.split('REFERENCES')\n",
    "            cleaned_paper = res[0]\n",
    "        \n",
    "        # remove line breaks / i.e. rejoin hyphenated words; i.e., find \"-\\n\"\n",
    "        cleaned_paper = cleaned_paper.replace(\"-\\n\", \"\")\n",
    "                \n",
    "        # remove URLs; this is a quick and dirty way to do it\n",
    "        cleaned_paper = re.sub(r'http\\S+', '', cleaned_paper)\n",
    "        \n",
    "        # remove all non-alpha to take care of math; replace with '' for now\n",
    "        rx = r'[^\\w\\n\\s\\-\\/]+'\n",
    "        #rx = r'[^a-zA-Z\\n\\s\\-]'\n",
    "        cleaned_paper = re.sub(rx, '', cleaned_paper)\n",
    "        cleaned_paper = re.sub(r'\\d', '', cleaned_paper)\n",
    "        # TODO clean up with dict\n",
    "        paper_as_tokens = cleaned_paper.split()\n",
    "        # There is a more efficient way to do this...but I don't care\n",
    "        missed_words = set()\n",
    "        print(\"Words removed from: \" + papername)\n",
    "        for idx, token in enumerate(paper_as_tokens):\n",
    "            if token.lower() not in dictionary and token.lower() not in missed_words:\n",
    "                missed_words.add(token.lower())\n",
    "                print(token.lower())\n",
    "                # remove bad token\n",
    "                paper_as_tokens[idx] = \"\"\n",
    "        f.close()\n",
    "    \n",
    "    #print(cleaned_paper)\n",
    "    # now overwrite the file\n",
    "    if os.path.isfile(full_filename):\n",
    "        print(\"Writing intermediate out file: {}\".format(papername))\n",
    "        f = open(full_filename, 'w')\n",
    "        f.write(cleaned_paper)\n",
    "        f.close()\n",
    "    \n",
    "    # set to true after completing manual cleaning\n",
    "    if finish_processing and os.path.isfile(full_filename):\n",
    "        cleaned_paper = ' '.join(paper_as_tokens)\n",
    "        print(\"Writing final cleaned file: {}\\n\".format(papername))\n",
    "        f = open(full_filename, 'w')\n",
    "        f.write(cleaned_paper)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 -- copying and modifying above; refactoring this would just be more annoying\n",
    "\n",
    "# for each paper -- run once, see which words are not in the dictionary,\n",
    "# add valid missing words to dictionary\n",
    "# run again to confirm caught appropriate missing words\n",
    "# set finish_processing=True to write the cleaned file\n",
    "\n",
    "license1 = \"\"\"Permission to make digital or hard copies of all or part of this work for personal or\n",
    "classroom use is granted without fee provided that copies are not made or distributed\n",
    "for profit or commercial advantage and that copies bear this notice and the full citation\n",
    "on the first page. Copyrights for components of this work owned by others than ACM\n",
    "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
    "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
    "fee. Request permissions from permissions@acm.org.\"\"\"\n",
    "\n",
    "proceedings = \"\"\"FAT* ’19, January 29–31, 2019, Atlanta, GA, USA\n",
    "© 2019 Association for Computing Machinery.\n",
    "ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00\"\"\"\n",
    "\n",
    "def clean_paper_2019(papername, paper_path, finish_processing=False):\n",
    "    full_filename = os.path.join(papers_path_root + \"2019\", papername)\n",
    "    # checking if it is a file, cleaning part done in one place\n",
    "    if os.path.isfile(full_filename):\n",
    "        # open file for reading\n",
    "        f = open(full_filename, 'r')\n",
    "        file_contents = f.read()\n",
    "        # for simplicity of running, make sure that after process once,\n",
    "        # can re-process/ is idempotent\n",
    "        \n",
    "        # remove \"ABSTRACT\"\n",
    "        cleaned_paper = file_contents\n",
    "        if \"ABSTRACT\" in cleaned_paper: # TODO make sure case insensitive\n",
    "            res = cleaned_paper.split(\"ABSTRACT\")\n",
    "            cleaned_paper = res[0] + \" \" + res[1]\n",
    "        \n",
    "        # remove license\n",
    "        if license1 in cleaned_paper:\n",
    "            res = cleaned_paper.split(license1)\n",
    "            cleaned_paper = res[0] + res[1]\n",
    "        \n",
    "        # remove proceedings\n",
    "        if proceedings in cleaned_paper:\n",
    "            res = cleaned_paper.split(proceedings)\n",
    "            cleaned_paper = res[0] + res[1]\n",
    "        \n",
    "        # remove acknowledgements\n",
    "        if 'Acknowledgments' in cleaned_paper:       \n",
    "            res = cleaned_paper.split('Acknowledgments')\n",
    "            cleaned_paper = res[0]\n",
    "        elif 'Acknowledgements'in cleaned_paper:\n",
    "            res = cleaned_paper.split('Acknowledgements')\n",
    "            cleaned_paper = res[0]\n",
    "        elif 'ACKNOWLEDGMENTS' in cleaned_paper:\n",
    "            res = cleaned_paper.split('ACKNOWLEDGMENTS')\n",
    "            cleaned_paper = res[0]\n",
    "        elif 'ACKNOWLEDGEMENTS' in cleaned_paper:\n",
    "            res = cleaned_paper.split('ACKNOWLEDGEMENTS')\n",
    "            cleaned_paper = res[0]\n",
    "        \n",
    "        # remove references, if the above didn't get them when removed acknowledgements\n",
    "        if 'References' in cleaned_paper:       \n",
    "            res = cleaned_paper.split('References')\n",
    "            cleaned_paper = res[0]\n",
    "        elif 'REFERENCES'in cleaned_paper:\n",
    "            res = cleaned_paper.split('REFERENCES')\n",
    "            cleaned_paper = res[0]\n",
    "        \n",
    "        # remove line breaks / i.e. rejoin hyphenated words; i.e., find \"-\\n\"\n",
    "        cleaned_paper = cleaned_paper.replace(\"-\\n\", \"\")\n",
    "        # remove ccs\n",
    "        cleaned_paper = cleaned_paper.replace(\"CCS\", \"\")\n",
    "                \n",
    "        # remove URLs; this is a quick and dirty way to do it\n",
    "        cleaned_paper = re.sub(r'http\\S+', '', cleaned_paper)\n",
    "        \n",
    "        # remove all non-alpha to take care of math; replace with '' for now\n",
    "        rx = r'[^\\w\\n\\s\\-\\/]+'\n",
    "        #rx = r'[^a-zA-Z\\n\\s\\-]'\n",
    "        cleaned_paper = re.sub(rx, '', cleaned_paper)\n",
    "        cleaned_paper = re.sub(r'\\d', '', cleaned_paper)\n",
    "        \n",
    "        # remove reference format\n",
    "        if \"ACM Reference Format\" in cleaned_paper:\n",
    "            res = cleaned_paper.split(\"ACM Reference Format\")\n",
    "            res2 = res[1].split(\"\"\"ACM New York NY USA pages\"\"\")\n",
    "            cleaned_paper = res[0] + \" \" + res2[1]\n",
    "        \n",
    "        # TODO clean up with dict\n",
    "        paper_as_tokens = cleaned_paper.split()\n",
    "        # There is a more efficient way to do this...but I don't care\n",
    "        missed_words = set()\n",
    "        print(\"Words removed from: \" + papername)\n",
    "        for idx, token in enumerate(paper_as_tokens):\n",
    "            if token.lower() not in dictionary and token.lower() not in missed_words:\n",
    "                missed_words.add(token.lower())\n",
    "                print(token.lower())\n",
    "                # remove bad token\n",
    "                paper_as_tokens[idx] = \"\"\n",
    "        f.close()\n",
    "        \n",
    "        #print(cleaned_paper)\n",
    "    # now overwrite the file\n",
    "    if os.path.isfile(full_filename):\n",
    "        print(\"Writing intermediate out file: {}\".format(papername))\n",
    "        f = open(full_filename, 'w')\n",
    "        f.write(cleaned_paper)\n",
    "        f.close()\n",
    "    \n",
    "    # set to true after completing manual cleaning\n",
    "    if finish_processing and os.path.isfile(full_filename):\n",
    "        cleaned_paper = ' '.join(paper_as_tokens)\n",
    "        print(\"Writing final cleaned file: {}\\n\".format(papername))\n",
    "        f = open(full_filename, 'w')\n",
    "        f.write(cleaned_paper)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up dictionary\n",
    "papers_path_root = '../../Data/TXTs/CleanedPapers_LDA/'\n",
    "\n",
    "dictionary = set()\n",
    "\n",
    "with open(papers_path_root + \"dictionary.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        if word and \"#\" not in word:\n",
    "            dictionary.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words removed from: 3287560.3287562.txt\n",
      "gradient-based\n",
      "two-layer\n",
      "relu\n",
      "learning-theoretic\n",
      "highlights\n",
      "gradients\n",
      "complementing\n",
      "heuristics\n",
      "reconstructing\n",
      "query-efficient\n",
      "computingmethodologiesmachine\n",
      "saliency\n",
      "f\n",
      "wx\n",
      "vectorw\n",
      "parametersw\n",
      "transitions\n",
      "wreluax\n",
      "rhd\n",
      "reluu\n",
      "maxu\n",
      "coordinatewise\n",
      "doi\n",
      "/\n",
      "smitha\n",
      "milli\n",
      "ludwig\n",
      "schmidt\n",
      "anca\n",
      "dragan\n",
      "moritz\n",
      "non-linear\n",
      "recovers\n",
      "fromoh\n",
      "logh\n",
      "theoh\n",
      "dh\n",
      "h\n",
      "ωdh\n",
      "enjoys\n",
      "anti-concentration\n",
      "exacerbating\n",
      "twolayer\n",
      "mnist\n",
      "cifar\n",
      "depth-\n",
      "k-layer\n",
      "withh\n",
      "okh\n",
      "ˆf\n",
      "hidden-layer\n",
      "activations\n",
      "maxai\n",
      "themodel\n",
      "andw\n",
      "useai\n",
      "i-th\n",
      "ofa\n",
      "rowsai\n",
      "andaj\n",
      "j\n",
      "aj\n",
      "reparameterizations\n",
      "scalingw\n",
      "δ\n",
      "notifies\n",
      "inodh\n",
      "дxiwia\n",
      "дx\n",
      "iax\n",
      "separating\n",
      "hyperplanes\n",
      "cells\n",
      "wiai\n",
      "amatrixz\n",
      "thatzpi\n",
      "orzpi\n",
      "maxzx\n",
      "learnmodelh\n",
      "ϵ\n",
      "recoverz\n",
      "recoversz\n",
      "recoverzh\n",
      "uv\n",
      "zi\n",
      "binarysearchtl\n",
      "tm\n",
      "xl\n",
      "tlv\n",
      "xm\n",
      "tmv\n",
      "xr\n",
      "trv\n",
      "rdh\n",
      "xh\n",
      "rankzx\n",
      "recovering\n",
      "дxiwiai\n",
      "hyperplane\n",
      "isolates\n",
      "recoverwiai\n",
      "betweenulv\n",
      "andulv\n",
      "thematrixz\n",
      "probabilityf\n",
      "towiai\n",
      "rowswiai\n",
      "tiv\n",
      "zpi\n",
      "k\n",
      "tkh\n",
      "binarysearch\n",
      "wkiaki\n",
      "orwkiaki\n",
      "tki\n",
      "minj\n",
      "terminates\n",
      "tkj\n",
      "eitherwkiaki\n",
      "rfx\n",
      "lv\n",
      "andv\n",
      "recovered\n",
      "recoverswiai\n",
      "tkl\n",
      "-th\n",
      "finishes\n",
      "scalars\n",
      "bu\n",
      "vectorsuv\n",
      "autv\n",
      "lower-bounded\n",
      "ji\n",
      "pti\n",
      "π\n",
      "upper-bound\n",
      "ª\n",
      "ino\n",
      "sgnwi\n",
      "zx\n",
      "full-rank\n",
      "izx\n",
      "izxh\n",
      "matrixm\n",
      "detm\n",
      "detzx\n",
      "det\n",
      "thusm\n",
      "amatrixx\n",
      "rhh\n",
      "inoh\n",
      "ℓg\n",
      "λℓm\n",
      "ℓj\n",
      "ℓm\n",
      "cross-entropy\n",
      "manipulated\n",
      "recoveredmodel\n",
      "smoothgrad\n",
      "denoising\n",
      "ground-truth\n",
      "-layer\n",
      "multinomial\n",
      "layers\n",
      "max-pool\n",
      "vgg\n",
      "resnet-\n",
      "xwhere\n",
      "unknownmodel\n",
      "followup\n",
      "wefind\n",
      "resnet\n",
      "referring\n",
      "degrades\n",
      "reconstructedmodel\n",
      "relatedwork\n",
      "tramèr\n",
      "preventing\n",
      "amodel\n",
      "instantiates\n",
      "near-optimal\n",
      "baum\n",
      "acknowledgements\n",
      "dge\n",
      "Writing intermediate out file: 3287560.3287562.txt\n"
     ]
    }
   ],
   "source": [
    "# We need to clean one at a time to make sure that we are spot-checking everything, sadly. I will do like 30 a day\n",
    "# using this a tracker, but also will keep a csv\n",
    "# when we are down, I will remove this, having updated the code above to clean everything appropriately\n",
    "# and will implement a for-loop for reproducibility\n",
    "\n",
    "# this is imperfect; we end up with some math stuff and weird spacing from the tables. But I think overall\n",
    "# it does a pretty good job\n",
    "\n",
    "restart = False\n",
    "\n",
    "if restart:\n",
    "    # 2018: papers 1-15\n",
    "    clean_paper_2018(\"binns18a\", papers_path_root, True) # 1\n",
    "    clean_paper_2018(\"barabas18a\", papers_path_root, True) # 2\n",
    "    clean_paper_2018(\"buolamwini18a\", papers_path_root, True) # 3\n",
    "    clean_paper_2018(\"burke18a\", papers_path_root, True) # 4\n",
    "    clean_paper_2018(\"chouldechova18a\", papers_path_root, True) # 5\n",
    "    clean_paper_2018(\"datta18a\", papers_path_root, True) # 6\n",
    "    clean_paper_2018(\"dwork18a\", papers_path_root, True) #7\n",
    "    clean_paper_2018(\"ekstrand18a\", papers_path_root, True) # 8 \n",
    "    clean_paper_2018(\"ekstrand18b\", papers_path_root, True) # 9\n",
    "    clean_paper_2018(\"ensign18a\", papers_path_root, True) # 10\n",
    "    clean_paper_2018(\"kamishima18a\", papers_path_root, True) # 11\n",
    "    clean_paper_2018(\"madaan18a\", papers_path_root, True) # 12\n",
    "    clean_paper_2018(\"menon18a\", papers_path_root, True) # 13\n",
    "    clean_paper_2018(\"phillips18a\", papers_path_root, True) # 14\n",
    "    clean_paper_2018(\"speicher18a\", papers_path_root, True) # 15\n",
    "    # 2019\n",
    "    # manually remove author list for each 2019 paper\n",
    "    clean_paper_2019(\"3287560.3287561.txt\", papers_path_root, True) # 16 \n",
    "\n",
    "clean_paper_2019(\"3287560.3287562.txt\", papers_path_root) # 17     \n",
    "\n",
    "#fns = os.listdir(papers_path_root + \"2019\")\n",
    "#fns.sort()\n",
    "#for fn in fns:\n",
    "#    print(papers_path_root + \"2019/\" + fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year = \"2018\"\n",
    "\n",
    "def make_paper_df(year, root_dir_path):\n",
    "    dir_name = papers_path_root + year\n",
    "    i = 1\n",
    "    data = []\n",
    "    for filename in os.listdir(dir_name):\n",
    "        full_filename = os.path.join(dir_name, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(full_filename):\n",
    "            f = open(full_filename, 'r')\n",
    "            file_contents = f.read()\n",
    "            #print(file_contents)\n",
    "            paper_id = \"{}-{}\".format(year, i)\n",
    "            data.append([paper_id, year, filename, file_contents])\n",
    "            f.close()\n",
    "            i = i + 1\n",
    "\n",
    "    return pd.DataFrame(data, columns = ['ID', 'Year', 'FileName', 'Text'])\n",
    "\n",
    "\n",
    "df_2018 = make_paper_df('2018', papers_path_root)\n",
    "df_2019 = make_paper_df('2019', papers_path_root)\n",
    "df_2020 = make_paper_df('2020', papers_path_root)\n",
    "df_2021 = make_paper_df('2021', papers_path_root)\n",
    "\n",
    "df_all_papers = df_2018.append(df_2019).append(df_2020).append(df_2021)\n",
    "df_all_papers.reset_index(inplace=True, drop=True) \n",
    "\n",
    "print(\"2018 total papers: {}\".format(len(df_2018.index)))\n",
    "print(\"2019 total papers: {}\".format(len(df_2019.index)))\n",
    "print(\"2020 total papers: {}\".format(len(df_2020.index)))\n",
    "print(\"2021 total papers: {}\".format(len(df_2021.index)))\n",
    "print(\"Total papers: {}\".format(len(df_all_papers.index)))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_papers.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Try topic modeling\n",
    "\n",
    "Process each paper into chunks and keep track of the year for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_texts = []\n",
    "original_texts = []\n",
    "training_years = []\n",
    "# If want to do topic breakdown / distribution per paper, then add training_ids list\n",
    "chunk_size = 200\n",
    "min_leftover_chunk_size = 20\n",
    "\n",
    "for i, r in df_all_papers.iterrows():\n",
    "    _chunks = [' '.join(r['Text'].split()[i:i+200]) for i in range(0, len(r['Text'].split()), chunk_size)]\n",
    "    # TODO -- consider custom stop words?\n",
    "    _processed_chunks = [lmw.process_string(c, remove_stop_words=True, remove_short_words=False).strip() for c in _chunks]\n",
    "    _processed_chunks = [c for c in _processed_chunks if len(c.split()) >= min_leftover_chunk_size]\n",
    "\n",
    "    for i, c in enumerate(_processed_chunks):\n",
    "        training_texts.append(c)\n",
    "        original_texts.append(_chunks[i])\n",
    "        training_years.append(r['Year'])\n",
    "\n",
    "len(training_texts), len(training_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20  \n",
    "output_directory_path = 'output' \n",
    "#path_to_mallet = '/Volumes/Passport-1/packages/mallet-2.0.8/bin/mallet'\n",
    "path_to_mallet = \"~/mallet-2.0.8/bin/mallet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keys, topic_distributions = lmw.quick_train_topic_model(path_to_mallet, \n",
    "                                                              output_directory_path, \n",
    "                                                              num_topics, \n",
    "                                                              training_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(topic_distributions) == len(training_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(topic_keys):\n",
    "    print(i, '\\t', ' '.join(t[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _topic in range(0, num_topics):\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('TOPIC ' + str(_topic) + ': ' + ' '.join(topic_keys[_topic][:5]))\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print()\n",
    "    for p, d in lmw.get_top_docs(original_texts, topic_distributions, topic_index=_topic, n=3):\n",
    "        print(round(p, 4), d)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bf6718e1fb94d37e4dcdce4af31e71b15d9d634835b5761d954c77044e0e9ad"
  },
  "kernelspec": {
   "display_name": "Anaconda (base)",
   "language": "python",
   "name": "anaconda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
